{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from csv import DictReader\n",
    "from math import exp, log, sqrt,pow\n",
    "import itertools\n",
    "import math\n",
    "from random import random,shuffle,uniform,seed\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_generator(path, no_norm=False, task='c'):\n",
    "    ''' Unfold compressed data into matrix.'''\n",
    "    data = open(path,'r')\n",
    "    for row in data:\n",
    "        row = row.strip().split(\" \")\n",
    "        y = float(row[0])\n",
    "        row = row[1:]\n",
    "        x = []\n",
    "        for feature in row:\n",
    "            feature = feature.split(\":\")\n",
    "            idx = int(feature[0])\n",
    "            value = float(feature[1])\n",
    "            x.append([idx,value])\n",
    "\n",
    "        if not no_norm:\n",
    "            r = 0.0\n",
    "            for i in range(len(x)):\n",
    "                r+=x[i][1]*x[i][1]\n",
    "            for i in range(len(x)):\n",
    "                x[i][1] /=r\n",
    "        # if task=='c':\n",
    "        #     if y ==0.0:\n",
    "        #         y = -1.0\n",
    "\n",
    "        yield x, y\n",
    "\n",
    "\n",
    "def dot(u,v):\n",
    "    u_v = 0.\n",
    "    len_u = len(u)\n",
    "    for idx in range(len_u):\n",
    "        uu = u[idx]\n",
    "        vv = v[idx]\n",
    "        u_v += uu*vv\n",
    "    return u_v\n",
    "\n",
    "def mse_loss_function(y,p):\n",
    "    return (y - p)**2\n",
    "\n",
    "def mae_loss_function(y,p):\n",
    "    y = exp(y)\n",
    "    p = exp(p)\n",
    "    return abs(y - p)\n",
    "\n",
    "def log_loss_function(y,p):\n",
    "    return -(y*log(p)+(1-y)*log(1-p))\n",
    "\n",
    "def exponential_loss_function(y,p):\n",
    "    return log(1+exp(-y*p))\n",
    "\n",
    "def sigmoid(inX):\n",
    "    return 1/(1+exp(-inX))\n",
    "\n",
    "def bounded_sigmoid(inX):\n",
    "    return 1. / (1. + exp(-max(min(inX, 35.), -35.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SGD(object):\n",
    "    def __init__(self, lr=0.001, momentum=0.9, nesterov=True, adam=False, l2=0.0, \\\n",
    "                 l2_fm=0.0, l2_bias=0.0, ini_stdev= 0.01, dropout=0.5, task='c', \\\n",
    "                 n_components=4, nb_epoch=5, interaction=False, no_norm=False):\n",
    "\n",
    "        self.W = []\n",
    "        self.V = []        \n",
    "        self.bias = uniform(-ini_stdev, ini_stdev)\n",
    "        self.n_components=n_components\n",
    "        self.lr = lr # learning rate\n",
    "        self.l2 = l2\n",
    "        self.l2_fm = l2_fm\n",
    "        self.l2_bias = l2_bias\n",
    "        self.momentum = momentum\n",
    "        self.nesterov = nesterov\n",
    "        self.adam = adam\n",
    "        self.nb_epoch = nb_epoch\n",
    "        self.ini_stdev = ini_stdev\n",
    "        self.task = task\n",
    "        self.interaction = interaction\n",
    "        self.dropout = dropout\n",
    "        self.no_norm = no_norm\n",
    "        if self.task!='c':\n",
    "            # self.loss_function = mse_loss_function\n",
    "            self.loss_function = mae_loss_function\n",
    "        else:\n",
    "            # self.loss_function = exponential_loss_function\n",
    "            self.loss_function = log_loss_function\n",
    "\n",
    "    def preload(self, train, test):\n",
    "        train = data_generator(train, self.no_norm, self.task)\n",
    "        dim = 0\n",
    "        count = 0\n",
    "        for x, y in train:\n",
    "            for i in x:\n",
    "                idx, value = i\n",
    "                if idx > dim:\n",
    "                    dim = idx\n",
    "            count+=1\n",
    "        print('Training samples:',count)\n",
    "        test = data_generator(test,self.no_norm,self.task)\n",
    "        count=0\n",
    "        for x,y in test:\n",
    "            for i in x:\n",
    "                idx,value = i\n",
    "                if idx > dim:\n",
    "                    dim = idx\n",
    "            count+=1\n",
    "        print('Testing samples:', count)\n",
    "        \n",
    "        dim = dim + 1\n",
    "        print(\"Number of features:\", dim)\n",
    "        \n",
    "        self.W = [uniform(-self.ini_stdev, self.ini_stdev) for _ in range(dim)]\n",
    "        self.Velocity_W = [0.0 for _ in range(dim)]\n",
    "        \n",
    "        \n",
    "        self.V = [[uniform(-self.ini_stdev, self.ini_stdev) for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        self.Velocity_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        \n",
    "        self.Velocity_bias = 0.0\n",
    "        \n",
    "        self.dim = dim\n",
    "        \n",
    "        \n",
    "    def adam_init(self):\n",
    "        self.iterations = 0\n",
    "        self.beta_1 = 0.9\n",
    "        self.beta_2 = 0.999\n",
    "        self.epsilon=1e-8\n",
    "        self.decay = 0.\n",
    "        self.inital_decay = self.decay \n",
    "\n",
    "        dim =self.dim\n",
    "\n",
    "        self.m_W = [0.0 for _ in range(dim)]\n",
    "        self.v_W = [0.0 for _ in range(dim)]\n",
    "\n",
    "        self.m_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "        self.v_V = [[0.0 for _ in range(self.n_components)] for _ in range(dim)]\n",
    "\n",
    "        self.m_bias = 0.0\n",
    "        self.v_bias = 0.0\n",
    "\n",
    "\n",
    "    def adam_update(self,lr,x,residual):\n",
    "\n",
    "        if 0. < self.dropout < 1.:\n",
    "            self.droupout_x(x)\n",
    "        \n",
    "        lr = self.lr\n",
    "        if self.inital_decay > 0:\n",
    "            lr *= (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "        t = self.iterations + 1\n",
    "\n",
    "        lr_t = lr * sqrt(1. - pow(self.beta_2, t)) / (1. - pow(self.beta_1, t))\n",
    "        \n",
    "        for sample in x:\n",
    "            idx,value = sample\n",
    "            g = residual*value\n",
    "\n",
    "            m = self.m_W[idx]\n",
    "            m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "\n",
    "            v = self.v_W[idx]\n",
    "            v_t = (self.beta_2 * v) + (1. - self.beta_2) * (g**2)\n",
    "\n",
    "            p = self.W[idx]\n",
    "            p_t = p - lr_t *m_t / (sqrt(v_t) + self.epsilon)\n",
    "\n",
    "            if self.l2>0:\n",
    "                p_t = p_t - lr_t*self.l2*p\n",
    "\n",
    "            self.m_W[idx] = m_t\n",
    "            self.v_W[idx] = v_t\n",
    "            self.W[idx] = p_t\n",
    "\n",
    "        if self.interaction:\n",
    "            self._adam_update_fm(lr_t,x,residual)\n",
    "\n",
    "\n",
    "        m = self.m_bias\n",
    "        m_t = (self.beta_1 * m) + (1. - self.beta_1)*residual\n",
    "\n",
    "        v = self.v_bias\n",
    "        v_t = (self.beta_2 * v) + (1. - self.beta_2)*(residual**2)\n",
    "\n",
    "        p = self.bias\n",
    "        p_t = p - lr_t * m_t / (sqrt(v_t) + self.epsilon)\n",
    "        if self.l2_bias > 0:\n",
    "            pt = pt - lr_t * self.l2_bias*p\n",
    "\n",
    "        self.m_bias = m_t\n",
    "        self.v_bias = v_t\n",
    "        self.bias = p_t\n",
    "\n",
    "        self.iterations+=1\n",
    "\n",
    "    def _adam_update_fm(self,lr_t,x,residual):\n",
    "        len_x = len(x)\n",
    "        sum_f_dict = self.sum_f_dict\n",
    "        n_components = self.n_components\n",
    "        for f in range(n_components):\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                v = self.V[idx_i][f]\n",
    "                sum_f = sum_f_dict[f]\n",
    "                g = (sum_f*value_i - v *value_i*value_i)*residual\n",
    "\n",
    "                m = self.m_V[idx_i][f]\n",
    "                m_t = (self.beta_1 * m) + (1. - self.beta_1) * g\n",
    "\n",
    "                v = self.v_V[idx_i][f]\n",
    "                v_t = (self.beta_2 * v) + (1. - self.beta_2) * (g**2)\n",
    "\n",
    "                p = self.V[idx_i][f]\n",
    "                p_t = p - lr_t * m_t / (sqrt(v_t) + self.epsilon)\n",
    "\n",
    "                if self.l2_fm>0:\n",
    "                    p_t = p_t - lr_t * self.l2_fm*p\n",
    "\n",
    "                self.m_V[idx_i][f] = m_t\n",
    "                self.v_V[idx_i][f] = v_t\n",
    "                self.V[idx_i][f] = p_t\n",
    "\n",
    "    def droupout_x(self, x):\n",
    "        new_x = []\n",
    "        for i, var in enumerate(x):\n",
    "            if random() > self.dropout:\n",
    "                del x[i]\n",
    "\n",
    "    def _predict_fm(self, x):\n",
    "        len_x = len(x)\n",
    "        n_components = self.n_components\n",
    "        pred = 0.0\n",
    "        self.sum_f_dict = {}\n",
    "        for f in range(n_components):\n",
    "            sum_f = 0.0\n",
    "            sum_sqr_f = 0.0\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                d = self.V[idx_i][f] * value_i\n",
    "                sum_f +=d\n",
    "                sum_sqr_f +=d*d\n",
    "            pred+= 0.5 * (sum_f*sum_f - sum_sqr_f);\n",
    "            self.sum_f_dict[f] = sum_f\n",
    "        return pred\n",
    "\n",
    "    def _predict_one(self, x):\n",
    "        pred = self.bias\n",
    "        # pred = 0.0\n",
    "        for idx,value in x:\n",
    "            pred+=self.W[idx]*value\n",
    "        \n",
    "        if self.interaction:\n",
    "            pred+=self._predict_fm(x)\n",
    "\n",
    "        if self.task=='c':\n",
    "            pred = bounded_sigmoid(pred)\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def _update_fm(self, lr, x, residual):\n",
    "        len_x = len(x)\n",
    "        sum_f_dict = self.sum_f_dict\n",
    "        n_components = self.n_components\n",
    "        for f in range(n_components):\n",
    "            for i in range(len_x):\n",
    "                idx_i,value_i = x[i]\n",
    "                sum_f = sum_f_dict[f]\n",
    "                v = self.V[idx_i][f]\n",
    "                grad = (sum_f*value_i - v *value_i*value_i)*residual\n",
    "                \n",
    "                self.Velocity_V[idx_i][f] = self.momentum * self.Velocity_V[idx_i][f] - lr * grad\n",
    "                if self.nesterov:\n",
    "                    self.Velocity_V[idx_i][f] = self.momentum * self.Velocity_V[idx_i][f] - lr * grad\n",
    "                self.V[idx_i][f] = self.V[idx_i][f] + self.Velocity_V[idx_i][f] - lr*self.l2_fm*self.V[idx_i][f]\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, lr, x, residual):\n",
    "\n",
    "        if 0. < self.dropout < 1.:\n",
    "            self.droupout_x(x)\n",
    "\n",
    "        for sample in x:\n",
    "            idx, value = sample\n",
    "            grad = residual * value\n",
    "            self.Velocity_W[idx] =  self.momentum * self.Velocity_W[idx] - lr * grad\n",
    "            if self.nesterov:\n",
    "                 self.Velocity_W[idx] = self.momentum * self.Velocity_W[idx] - lr * grad\n",
    "            self.W[idx] = self.W[idx] + self.Velocity_W[idx] - lr * self.l2 * self.W[idx]\n",
    "            \n",
    "        if self.interaction:\n",
    "            self._update_fm(lr, x, residual)\n",
    "\n",
    "        self.Velocity_bias = self.momentum * self.Velocity_bias - lr * residual\n",
    "        if self.nesterov:\n",
    "            self.Velocity_bias = self.momentum * self.Velocity_bias - lr * residual\n",
    "        self.bias = self.bias + self.Velocity_bias - lr * self.l2_bias * self.bias\n",
    "\n",
    "    def predict(self, path, out):\n",
    "\n",
    "        data = data_generator(path, self.no_norm,self.task)\n",
    "        y_preds =[]\n",
    "        with open(out, 'w') as outfile:\n",
    "            ID = 0\n",
    "            outfile.write('%s,%s\\n' % ('test_id', 'is_duplicate'))\n",
    "            for d in data:\n",
    "                x, y = d\n",
    "                p = self._predict_one(x)\n",
    "                outfile.write('%s,%s\\n' % (ID, str(p)))\n",
    "                ID+=1\n",
    "\n",
    "\n",
    "    def validate(self, path):\n",
    "        data = data_generator(path, self.no_norm, self.task)\n",
    "        loss = 0.0\n",
    "        count = 0.0\n",
    "\n",
    "        for d in data:\n",
    "            x,y = d\n",
    "            p = self._predict_one(x)\n",
    "            loss+=self.loss_function(y,p)\n",
    "            count+=1\n",
    "        return loss/count\n",
    "\n",
    "    def save_weights(self):\n",
    "        weights = []\n",
    "        weights.append(self.W)\n",
    "        weights.append(self.V)\n",
    "        weights.append(self.bias)\n",
    "        # weights.append(self.Velocity_W)\n",
    "        # weights.append(self.Velocity_V)\n",
    "        weights.append(self.dim)\n",
    "        pickle.dump(weights, open('sgd_fm.pkl','wb'))\n",
    "\n",
    "    def load_weights(self):\n",
    "        weights = pickle.load(open('sgd_fm.pkl','rb'))\n",
    "        self.W = weights[0]\n",
    "        self.V = weights[1]\n",
    "        self.bias = weights[2]\n",
    "        # self.Velocity_W = weights[3]\n",
    "        # self.Velocity_V = weights[4]\n",
    "        self.dim = weights[3]\n",
    "        \n",
    "\n",
    "    def train(self, path, valid_path=None, in_memory=False):\n",
    "\n",
    "        start = datetime.now()\n",
    "        lr = self.lr\n",
    "        if self.adam:\n",
    "            self.adam_init()\n",
    "            self.update = self.adam_update\n",
    "\n",
    "        if in_memory:\n",
    "            data = data_generator(path, self.no_norm, self.task) # Unfold compressed data into matrix.\n",
    "            data = [d for d in data]\n",
    "            \n",
    "        best_loss = 999999\n",
    "        best_epoch = 0\n",
    "        early_stop_count = 0\n",
    "        for epoch in range(1, self.nb_epoch+1): # Training through the epochs.\n",
    "            if not in_memory:\n",
    "                data = data_generator(path, self.no_norm, self.task)\n",
    "            train_loss = 0.0\n",
    "            train_count = 0\n",
    "            for x, y in data: # Training through the whole batch.\n",
    "                p = self._predict_one(x)\n",
    "                if self.task != 'c':                    \n",
    "                    residual = -(y-p)\n",
    "                else:\n",
    "                    # residual = -y*(1.0-1.0/(1.0+exp(-y*p)));\n",
    "                    residual = -(y-p)\n",
    "\n",
    "                self.update(lr, x, residual)\n",
    "                if train_count % 50000 == 0:\n",
    "                    if train_count == 0:\n",
    "                        print('\\ttrain_count: %s, current loss: %.6f'%(train_count, 0.0))\n",
    "                    else:\n",
    "                        print('\\ttrain_count: %s, current loss: %.6f'%(train_count, train_loss/train_count))\n",
    "\n",
    "                train_loss += self.loss_function(y,p)\n",
    "                train_count += 1\n",
    "\n",
    "            epoch_end = datetime.now()\n",
    "            duration = epoch_end - start\n",
    "            \n",
    "            # Early Stopping: save weights of the best epoch.\n",
    "            if valid_path:\n",
    "                valid_loss = self.validate(valid_path)\n",
    "                print('Epoch: %s, train loss: %.6f, valid loss: %.6f, time: %s'%(epoch, train_loss/train_count,valid_loss, duration))\n",
    "                print('early_stop_count: ', early_stop_count)\n",
    "                if valid_loss < best_loss:\n",
    "                    best_loss = valid_loss\n",
    "                    self.save_weights()\n",
    "                    early_stop_count = 0                    \n",
    "                    print('save_weights')\n",
    "                else:\n",
    "                    early_stop_count = early_stop_count + 1\n",
    "\n",
    "            else:\n",
    "                print('Epoch: %s, train loss: %.6f, time: %s'%(epoch,train_loss/train_count,duration))\n",
    "            \n",
    "            if early_stop_count >= 3:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"../../kaggle-quora/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.001, \\\n",
    "          adam=True, \\\n",
    "          dropout=0.8, \\\n",
    "          l2=0.00, \\\n",
    "          l2_fm=0.00, \\\n",
    "          task='c', \\\n",
    "          n_components=1, \\\n",
    "          nb_epoch=30, \\\n",
    "          interaction=True, \\\n",
    "          no_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 624388\n",
      "Testing samples: 156098\n",
      "Number of features: 3073589\n",
      "CPU times: user 1min 45s, sys: 1.78 s, total: 1min 47s\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sgd.preload(path + 'train_tfidf_jacad_magic_fold_1.svm', path + 'val_tfidf_jacad_magic_fold_1.svm') # 3073539"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.230001\n",
      "\ttrain_count: 100000, current loss: 0.115000\n",
      "\ttrain_count: 150000, current loss: 0.288468\n",
      "\ttrain_count: 200000, current loss: 0.293488\n",
      "\ttrain_count: 250000, current loss: 0.234790\n",
      "\ttrain_count: 300000, current loss: 0.195659\n",
      "\ttrain_count: 350000, current loss: 0.167707\n",
      "\ttrain_count: 400000, current loss: 0.211577\n",
      "\ttrain_count: 450000, current loss: 0.218435\n",
      "\ttrain_count: 500000, current loss: 0.196592\n",
      "\ttrain_count: 550000, current loss: 0.178720\n",
      "\ttrain_count: 600000, current loss: 0.163827\n",
      "Epoch: 1, train loss: 0.157428, valid loss: 4.636476, time: 0:04:42.493119\n",
      "early_stop_count:  0\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.621771\n",
      "\ttrain_count: 100000, current loss: 0.310886\n",
      "\ttrain_count: 150000, current loss: 0.332854\n",
      "\ttrain_count: 200000, current loss: 0.324755\n",
      "\ttrain_count: 250000, current loss: 0.259804\n",
      "\ttrain_count: 300000, current loss: 0.216503\n",
      "\ttrain_count: 350000, current loss: 0.185574\n",
      "\ttrain_count: 400000, current loss: 0.217451\n",
      "\ttrain_count: 450000, current loss: 0.226738\n",
      "\ttrain_count: 500000, current loss: 0.204064\n",
      "\ttrain_count: 550000, current loss: 0.185513\n",
      "\ttrain_count: 600000, current loss: 0.170054\n",
      "Epoch: 2, train loss: 0.163412, valid loss: 4.873663, time: 0:09:30.843494\n",
      "early_stop_count:  0\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.592940\n",
      "\ttrain_count: 100000, current loss: 0.296471\n",
      "\ttrain_count: 150000, current loss: 0.305641\n",
      "\ttrain_count: 200000, current loss: 0.304034\n",
      "\ttrain_count: 250000, current loss: 0.243227\n",
      "\ttrain_count: 300000, current loss: 0.202689\n",
      "\ttrain_count: 350000, current loss: 0.173734\n",
      "\ttrain_count: 400000, current loss: 0.203952\n",
      "\ttrain_count: 450000, current loss: 0.213211\n",
      "\ttrain_count: 500000, current loss: 0.191890\n",
      "\ttrain_count: 550000, current loss: 0.174445\n",
      "\ttrain_count: 600000, current loss: 0.159908\n",
      "Epoch: 3, train loss: 0.153662, valid loss: 5.041532, time: 0:14:16.267989\n",
      "early_stop_count:  1\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.575682\n",
      "\ttrain_count: 100000, current loss: 0.287845\n",
      "\ttrain_count: 150000, current loss: 0.284411\n",
      "\ttrain_count: 200000, current loss: 0.288519\n",
      "\ttrain_count: 250000, current loss: 0.230823\n",
      "\ttrain_count: 300000, current loss: 0.192352\n",
      "\ttrain_count: 350000, current loss: 0.164873\n",
      "\ttrain_count: 400000, current loss: 0.196537\n",
      "\ttrain_count: 450000, current loss: 0.206302\n",
      "\ttrain_count: 500000, current loss: 0.185672\n",
      "\ttrain_count: 550000, current loss: 0.168793\n",
      "\ttrain_count: 600000, current loss: 0.154727\n",
      "Epoch: 4, train loss: 0.148684, valid loss: 5.209681, time: 0:19:00.484069\n",
      "early_stop_count:  2\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 1.066722\n",
      "\ttrain_count: 100000, current loss: 0.533370\n",
      "\ttrain_count: 150000, current loss: 0.355580\n",
      "\ttrain_count: 200000, current loss: 0.436787\n",
      "\ttrain_count: 250000, current loss: 0.350906\n",
      "\ttrain_count: 300000, current loss: 0.292422\n",
      "\ttrain_count: 350000, current loss: 0.250647\n",
      "\ttrain_count: 400000, current loss: 0.271975\n",
      "\ttrain_count: 450000, current loss: 0.275573\n",
      "\ttrain_count: 500000, current loss: 0.248016\n",
      "\ttrain_count: 550000, current loss: 0.225469\n",
      "\ttrain_count: 600000, current loss: 0.206680\n",
      "Epoch: 1, train loss: 0.198607, valid loss: 4.856266, time: 0:04:30.923704\n",
      "early_stop_count:  0\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.631473\n",
      "\ttrain_count: 100000, current loss: 0.315756\n",
      "\ttrain_count: 150000, current loss: 0.210504\n",
      "\ttrain_count: 200000, current loss: 0.321315\n",
      "\ttrain_count: 250000, current loss: 0.258287\n",
      "\ttrain_count: 300000, current loss: 0.215239\n",
      "\ttrain_count: 350000, current loss: 0.184491\n",
      "\ttrain_count: 400000, current loss: 0.212466\n",
      "\ttrain_count: 450000, current loss: 0.221638\n",
      "\ttrain_count: 500000, current loss: 0.199475\n",
      "\ttrain_count: 550000, current loss: 0.181340\n",
      "\ttrain_count: 600000, current loss: 0.166229\n",
      "Epoch: 2, train loss: 0.159736, valid loss: 5.066910, time: 0:09:21.969382\n",
      "early_stop_count:  0\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.631913\n",
      "\ttrain_count: 100000, current loss: 0.315998\n",
      "\ttrain_count: 150000, current loss: 0.210665\n",
      "\ttrain_count: 200000, current loss: 0.325419\n",
      "\ttrain_count: 250000, current loss: 0.261611\n",
      "\ttrain_count: 300000, current loss: 0.218009\n",
      "\ttrain_count: 350000, current loss: 0.186865\n",
      "\ttrain_count: 400000, current loss: 0.215658\n",
      "\ttrain_count: 450000, current loss: 0.225308\n",
      "\ttrain_count: 500000, current loss: 0.202778\n",
      "\ttrain_count: 550000, current loss: 0.184343\n",
      "\ttrain_count: 600000, current loss: 0.168981\n",
      "Epoch: 3, train loss: 0.162381, valid loss: 5.255867, time: 0:14:11.046185\n",
      "early_stop_count:  1\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.654169\n",
      "\ttrain_count: 100000, current loss: 0.327147\n",
      "\ttrain_count: 150000, current loss: 0.218098\n",
      "\ttrain_count: 200000, current loss: 0.332109\n",
      "\ttrain_count: 250000, current loss: 0.267125\n",
      "\ttrain_count: 300000, current loss: 0.222604\n",
      "\ttrain_count: 350000, current loss: 0.190804\n",
      "\ttrain_count: 400000, current loss: 0.219728\n",
      "\ttrain_count: 450000, current loss: 0.229493\n",
      "\ttrain_count: 500000, current loss: 0.206546\n",
      "\ttrain_count: 550000, current loss: 0.187769\n",
      "\ttrain_count: 600000, current loss: 0.172122\n",
      "Epoch: 4, train loss: 0.165399, valid loss: 5.434781, time: 0:18:59.541275\n",
      "early_stop_count:  2\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.902198\n",
      "\ttrain_count: 100000, current loss: 0.552777\n",
      "\ttrain_count: 150000, current loss: 0.368518\n",
      "\ttrain_count: 200000, current loss: 0.276388\n",
      "\ttrain_count: 250000, current loss: 0.221111\n",
      "\ttrain_count: 300000, current loss: 0.279570\n",
      "\ttrain_count: 350000, current loss: 0.239638\n",
      "\ttrain_count: 400000, current loss: 0.249281\n",
      "\ttrain_count: 450000, current loss: 0.253418\n",
      "\ttrain_count: 500000, current loss: 0.228076\n",
      "\ttrain_count: 550000, current loss: 0.207342\n",
      "\ttrain_count: 600000, current loss: 0.190063\n",
      "Epoch: 1, train loss: 0.182640, valid loss: 5.061485, time: 0:04:29.163441\n",
      "early_stop_count:  0\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.497053\n",
      "\ttrain_count: 100000, current loss: 0.351225\n",
      "\ttrain_count: 150000, current loss: 0.234150\n",
      "\ttrain_count: 200000, current loss: 0.175613\n",
      "\ttrain_count: 250000, current loss: 0.140490\n",
      "\ttrain_count: 300000, current loss: 0.213796\n",
      "\ttrain_count: 350000, current loss: 0.183260\n",
      "\ttrain_count: 400000, current loss: 0.200490\n",
      "\ttrain_count: 450000, current loss: 0.210910\n",
      "\ttrain_count: 500000, current loss: 0.189819\n",
      "\ttrain_count: 550000, current loss: 0.172563\n",
      "\ttrain_count: 600000, current loss: 0.158183\n",
      "Epoch: 2, train loss: 0.152004, valid loss: 5.268120, time: 0:09:21.113746\n",
      "early_stop_count:  0\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.516709\n",
      "\ttrain_count: 100000, current loss: 0.365101\n",
      "\ttrain_count: 150000, current loss: 0.243401\n",
      "\ttrain_count: 200000, current loss: 0.182551\n",
      "\ttrain_count: 250000, current loss: 0.146041\n",
      "\ttrain_count: 300000, current loss: 0.222706\n",
      "\ttrain_count: 350000, current loss: 0.190896\n",
      "\ttrain_count: 400000, current loss: 0.206919\n",
      "\ttrain_count: 450000, current loss: 0.217390\n",
      "\ttrain_count: 500000, current loss: 0.195654\n",
      "\ttrain_count: 550000, current loss: 0.177867\n",
      "\ttrain_count: 600000, current loss: 0.163045\n",
      "Epoch: 3, train loss: 0.156677, valid loss: 5.403894, time: 0:14:10.317451\n",
      "early_stop_count:  1\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.528459\n",
      "\ttrain_count: 100000, current loss: 0.368982\n",
      "\ttrain_count: 150000, current loss: 0.245990\n",
      "\ttrain_count: 200000, current loss: 0.184493\n",
      "\ttrain_count: 250000, current loss: 0.147594\n",
      "\ttrain_count: 300000, current loss: 0.218408\n",
      "\ttrain_count: 350000, current loss: 0.187255\n",
      "\ttrain_count: 400000, current loss: 0.191170\n",
      "\ttrain_count: 450000, current loss: 0.201368\n",
      "\ttrain_count: 500000, current loss: 0.181290\n",
      "\ttrain_count: 550000, current loss: 0.164822\n",
      "\ttrain_count: 600000, current loss: 0.151091\n",
      "Epoch: 4, train loss: 0.145190, valid loss: 4.832479, time: 0:18:59.613580\n",
      "early_stop_count:  2\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.410476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_count: 100000, current loss: 0.304521\n",
      "\ttrain_count: 150000, current loss: 0.203133\n",
      "\ttrain_count: 200000, current loss: 0.152385\n",
      "\ttrain_count: 250000, current loss: 0.121910\n",
      "\ttrain_count: 300000, current loss: 0.185604\n",
      "\ttrain_count: 350000, current loss: 0.159206\n",
      "\ttrain_count: 400000, current loss: 0.164810\n",
      "\ttrain_count: 450000, current loss: 0.176579\n",
      "\ttrain_count: 500000, current loss: 0.159034\n",
      "\ttrain_count: 550000, current loss: 0.144604\n",
      "\ttrain_count: 600000, current loss: 0.132578\n",
      "Epoch: 5, train loss: 0.127407, valid loss: 4.793581, time: 0:23:50.988636\n",
      "early_stop_count:  0\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.397990\n",
      "\ttrain_count: 100000, current loss: 0.294994\n",
      "\ttrain_count: 150000, current loss: 0.196844\n",
      "\ttrain_count: 200000, current loss: 0.147692\n",
      "\ttrain_count: 250000, current loss: 0.118157\n",
      "\ttrain_count: 300000, current loss: 0.180824\n",
      "\ttrain_count: 350000, current loss: 0.155137\n",
      "\ttrain_count: 400000, current loss: 0.161075\n",
      "\ttrain_count: 450000, current loss: 0.173087\n",
      "\ttrain_count: 500000, current loss: 0.155904\n",
      "\ttrain_count: 550000, current loss: 0.141765\n",
      "\ttrain_count: 600000, current loss: 0.129986\n",
      "Epoch: 6, train loss: 0.124919, valid loss: 4.864793, time: 0:28:42.625229\n",
      "early_stop_count:  0\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.402725\n",
      "\ttrain_count: 100000, current loss: 0.294954\n",
      "\ttrain_count: 150000, current loss: 0.196855\n",
      "\ttrain_count: 200000, current loss: 0.147716\n",
      "\ttrain_count: 250000, current loss: 0.118179\n",
      "\ttrain_count: 300000, current loss: 0.180809\n",
      "\ttrain_count: 350000, current loss: 0.155150\n",
      "\ttrain_count: 400000, current loss: 0.161191\n",
      "\ttrain_count: 450000, current loss: 0.172814\n",
      "\ttrain_count: 500000, current loss: 0.155673\n",
      "\ttrain_count: 550000, current loss: 0.141563\n",
      "\ttrain_count: 600000, current loss: 0.129809\n",
      "Epoch: 7, train loss: 0.124752, valid loss: 4.948422, time: 0:33:32.530259\n",
      "early_stop_count:  1\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.413591\n",
      "\ttrain_count: 100000, current loss: 0.301352\n",
      "\ttrain_count: 150000, current loss: 0.201163\n",
      "\ttrain_count: 200000, current loss: 0.150963\n",
      "\ttrain_count: 250000, current loss: 0.120781\n",
      "\ttrain_count: 300000, current loss: 0.184575\n",
      "\ttrain_count: 350000, current loss: 0.158409\n",
      "\ttrain_count: 400000, current loss: 0.164030\n",
      "\ttrain_count: 450000, current loss: 0.175859\n",
      "\ttrain_count: 500000, current loss: 0.158423\n",
      "\ttrain_count: 550000, current loss: 0.144070\n",
      "\ttrain_count: 600000, current loss: 0.132114\n",
      "Epoch: 8, train loss: 0.126970, valid loss: 5.010746, time: 0:38:23.120200\n",
      "early_stop_count:  2\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.746204\n",
      "\ttrain_count: 100000, current loss: 0.467308\n",
      "\ttrain_count: 150000, current loss: 0.311746\n",
      "\ttrain_count: 200000, current loss: 0.233881\n",
      "\ttrain_count: 250000, current loss: 0.187109\n",
      "\ttrain_count: 300000, current loss: 0.218492\n",
      "\ttrain_count: 350000, current loss: 0.221380\n",
      "\ttrain_count: 400000, current loss: 0.193766\n",
      "\ttrain_count: 450000, current loss: 0.172298\n",
      "\ttrain_count: 500000, current loss: 0.204075\n",
      "\ttrain_count: 550000, current loss: 0.188098\n",
      "\ttrain_count: 600000, current loss: 0.172506\n",
      "Epoch: 1, train loss: 0.165793, valid loss: 4.371656, time: 0:04:30.950938\n",
      "early_stop_count:  0\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.333006\n",
      "\ttrain_count: 100000, current loss: 0.260663\n",
      "\ttrain_count: 150000, current loss: 0.174043\n",
      "\ttrain_count: 200000, current loss: 0.130632\n",
      "\ttrain_count: 250000, current loss: 0.104519\n",
      "\ttrain_count: 300000, current loss: 0.149323\n",
      "\ttrain_count: 350000, current loss: 0.161734\n",
      "\ttrain_count: 400000, current loss: 0.141584\n",
      "\ttrain_count: 450000, current loss: 0.125933\n",
      "\ttrain_count: 500000, current loss: 0.163370\n",
      "\ttrain_count: 550000, current loss: 0.151105\n",
      "\ttrain_count: 600000, current loss: 0.138618\n",
      "Epoch: 2, train loss: 0.133236, valid loss: 4.424668, time: 0:09:25.416931\n",
      "early_stop_count:  0\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.337805\n",
      "\ttrain_count: 100000, current loss: 0.263302\n",
      "\ttrain_count: 150000, current loss: 0.175860\n",
      "\ttrain_count: 200000, current loss: 0.132019\n",
      "\ttrain_count: 250000, current loss: 0.105636\n",
      "\ttrain_count: 300000, current loss: 0.149728\n",
      "\ttrain_count: 350000, current loss: 0.161697\n",
      "\ttrain_count: 400000, current loss: 0.141556\n",
      "\ttrain_count: 450000, current loss: 0.125926\n",
      "\ttrain_count: 500000, current loss: 0.163528\n",
      "\ttrain_count: 550000, current loss: 0.151329\n",
      "\ttrain_count: 600000, current loss: 0.138834\n",
      "Epoch: 3, train loss: 0.133447, valid loss: 4.504203, time: 0:14:17.084427\n",
      "early_stop_count:  1\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.347148\n",
      "\ttrain_count: 100000, current loss: 0.269928\n",
      "\ttrain_count: 150000, current loss: 0.180344\n",
      "\ttrain_count: 200000, current loss: 0.135412\n",
      "\ttrain_count: 250000, current loss: 0.108357\n",
      "\ttrain_count: 300000, current loss: 0.153148\n",
      "\ttrain_count: 350000, current loss: 0.164814\n",
      "\ttrain_count: 400000, current loss: 0.144294\n",
      "\ttrain_count: 450000, current loss: 0.128381\n",
      "\ttrain_count: 500000, current loss: 0.167650\n",
      "\ttrain_count: 550000, current loss: 0.155190\n",
      "\ttrain_count: 600000, current loss: 0.142398\n",
      "Epoch: 4, train loss: 0.136879, valid loss: 4.587262, time: 0:19:08.835863\n",
      "early_stop_count:  2\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.634713\n",
      "\ttrain_count: 100000, current loss: 0.409368\n",
      "\ttrain_count: 150000, current loss: 0.273208\n",
      "\ttrain_count: 200000, current loss: 0.205019\n",
      "\ttrain_count: 250000, current loss: 0.164030\n",
      "\ttrain_count: 300000, current loss: 0.197241\n",
      "\ttrain_count: 350000, current loss: 0.208692\n",
      "\ttrain_count: 400000, current loss: 0.182759\n",
      "\ttrain_count: 450000, current loss: 0.162520\n",
      "\ttrain_count: 500000, current loss: 0.146326\n",
      "\ttrain_count: 550000, current loss: 0.178174\n",
      "\ttrain_count: 600000, current loss: 0.163517\n",
      "Epoch: 1, train loss: 0.157169, valid loss: 3.891697, time: 0:04:30.531732\n",
      "early_stop_count:  0\n",
      "save_weights\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.274006\n",
      "\ttrain_count: 100000, current loss: 0.234220\n",
      "\ttrain_count: 150000, current loss: 0.156504\n",
      "\ttrain_count: 200000, current loss: 0.117517\n",
      "\ttrain_count: 250000, current loss: 0.094035\n",
      "\ttrain_count: 300000, current loss: 0.139684\n",
      "\ttrain_count: 350000, current loss: 0.160062\n",
      "\ttrain_count: 400000, current loss: 0.140236\n",
      "\ttrain_count: 450000, current loss: 0.124737\n",
      "\ttrain_count: 500000, current loss: 0.112331\n",
      "\ttrain_count: 550000, current loss: 0.148612\n",
      "\ttrain_count: 600000, current loss: 0.136436\n",
      "Epoch: 2, train loss: 0.131153, valid loss: 3.944206, time: 0:09:23.432057\n",
      "early_stop_count:  0\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.279496\n",
      "\ttrain_count: 100000, current loss: 0.240305\n",
      "\ttrain_count: 150000, current loss: 0.160635\n",
      "\ttrain_count: 200000, current loss: 0.120648\n",
      "\ttrain_count: 250000, current loss: 0.096545\n",
      "\ttrain_count: 300000, current loss: 0.142887\n",
      "\ttrain_count: 350000, current loss: 0.164235\n",
      "\ttrain_count: 400000, current loss: 0.143913\n",
      "\ttrain_count: 450000, current loss: 0.128025\n",
      "\ttrain_count: 500000, current loss: 0.115305\n",
      "\ttrain_count: 550000, current loss: 0.152743\n",
      "\ttrain_count: 600000, current loss: 0.140243\n",
      "Epoch: 3, train loss: 0.134820, valid loss: 4.025814, time: 0:14:13.129632\n",
      "early_stop_count:  1\n",
      "\ttrain_count: 0, current loss: 0.000000\n",
      "\ttrain_count: 50000, current loss: 0.287906\n",
      "\ttrain_count: 100000, current loss: 0.247843\n",
      "\ttrain_count: 150000, current loss: 0.165737\n",
      "\ttrain_count: 200000, current loss: 0.124512\n",
      "\ttrain_count: 250000, current loss: 0.099644\n",
      "\ttrain_count: 300000, current loss: 0.145491\n",
      "\ttrain_count: 350000, current loss: 0.166518\n",
      "\ttrain_count: 400000, current loss: 0.145935\n",
      "\ttrain_count: 450000, current loss: 0.129837\n",
      "\ttrain_count: 500000, current loss: 0.116948\n",
      "\ttrain_count: 550000, current loss: 0.155555\n",
      "\ttrain_count: 600000, current loss: 0.142834\n",
      "Epoch: 4, train loss: 0.137314, valid loss: 4.089083, time: 0:19:02.731434\n",
      "early_stop_count:  2\n"
     ]
    }
   ],
   "source": [
    "# Train to generate 5 folds as features for layer 2.\n",
    "\n",
    "for i in range(1, 6, 1):\n",
    "    sgd.train(path + 'train_tfidf_jacad_magic_fold_{}.svm'.format(i), \\\n",
    "              path + 'val_tfidf_jacad_magic_fold_{}.svm'.format(i), \\\n",
    "              in_memory=False)\n",
    "    sgd.load_weights()\n",
    "    sgd.predict(path+'val_tfidf_jacad_magic_fold_{}.svm'.format(i), out='fm_layer_1_fold{}.csv'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26049346353890346\n"
     ]
    }
   ],
   "source": [
    "sgd.load_weights()\n",
    "sgd.predict(path+'X_test_tfidf_jacad_magic.svm', out='valid_jacad_magic.csv')\n",
    "print(sgd.validate(path+'X_test_tfidf_jacad_magic.svm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd.predict(path+'X_t_tfidf_jacad_magic.svm', out='pred_jacad_magic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Previous result with default features.\n",
    "# 0.2605703106856683"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing out to understand the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train = data_generator(path+'X_tfidf.svm', False, 'c')\n",
    "data = open(path+'X_tfidf.svm','r')\n",
    "print(data)\n",
    "\n",
    "i=0\n",
    "for row in data:\n",
    "    print(len(row))\n",
    "    row = row.strip().split(\" \")\n",
    "    print(row)\n",
    "    \n",
    "    y = float(row[0])\n",
    "    print('y: ', y)\n",
    "\n",
    "    row = row[1:]\n",
    "    print('row: ', row)\n",
    "    \n",
    "    x = []\n",
    "    for feature in row:\n",
    "        feature = feature.split(\":\")\n",
    "        idx = int(feature[0])\n",
    "        value = float(feature[1])\n",
    "        x.append([idx, value])\n",
    "    \n",
    "    if i >= 0:\n",
    "        break\n",
    "    i = i + 1\n",
    "    \n",
    "\n",
    "\n",
    "#     if not no_norm:\n",
    "#         r = 0.0\n",
    "#         for i in range(len(x)):\n",
    "#             r+=x[i][1]*x[i][1]\n",
    "#         for i in range(len(x)):\n",
    "#             x[i][1] /=r\n",
    "\n",
    "#     yield x, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
